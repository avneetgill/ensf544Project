{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 \n",
    "\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load the pickle data into dataframes from the Output folder one directory above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All Bug Reports are Loaded. ***\n",
      "*** All Source Codes are Loaded. ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'217'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadEverything():\n",
    "    all_projects_bugreports = pd.read_pickle('Output/allBugReports.pickle')\n",
    "    print(\"*** All Bug Reports are Loaded. ***\")\n",
    "    all_projects_source_codes = pd.read_pickle('Output/allSourceCodes.pickle')\n",
    "    print(\"*** All Source Codes are Loaded. ***\")\n",
    "    return all_projects_bugreports, all_projects_source_codes\n",
    "\n",
    "all_projects_bugreports, all_projects_source_codes = loadEverything()\n",
    "display(all_projects_bugreports.iloc[0])\n",
    "display(all_projects_source_codes.iloc[2])\n",
    "display(all_projects_bugreports.iloc[0].name)\n",
    "display(all_projects_source_codes.iloc[2].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['org.apache.commons.collections.map.flat3map.java',\n",
       "       'org.apache.commons.collections.map.testflat3map.java'],\n",
       "      dtype='<U52')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'org.apache.commons.collections.list.transformedlist.java'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# need to get the file names in the bug report an source code to be the same\n",
    "\n",
    "# there is no index connection between bugreports index (.name) and the index of the source code\n",
    "display(all_projects_bugreports.iloc[0].fix)\n",
    "display(all_projects_source_codes.iloc[217].filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing composite varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#remove next line characters:\n",
    "def remove_new_lines(text):\n",
    "#     return str(x).replace('\\n', '').replace('*', '').replace('/', '').replace('\\\\','').replace('\\t','')\n",
    "    text = str(text)\n",
    "    COMBINE_WHITE_SPACE = re.compile(r\"(?a:\\s+)\")\n",
    "    text = COMBINE_WHITE_SPACE.sub(' ', text)\n",
    "    return text.replace('*', '').replace('/', '').replace('\\\\','')\n",
    "    \n",
    "# clean up the various white space and remove some *\n",
    "def clean_new_lines_source_code(df):\n",
    "    df.unprocessed_code = df.unprocessed_code.apply(remove_new_lines)\n",
    "    return df\n",
    "\n",
    "# clean up the description and summary, they will both be used for the query\n",
    "def clean_new_lines_bug_report(df):\n",
    "    df.summary = df.summary.apply(remove_new_lines)\n",
    "    df['description'] = df['description'].astype('|S')\n",
    "    df.description = df.description.apply(remove_new_lines)\n",
    "    return df\n",
    "\n",
    "# need to reformat the source code so it can be compared to the bug reports fix array\n",
    "def format_sc_filename(x):\n",
    "  \n",
    "    x = x.split('apache')\n",
    "    if len(x) != 2:\n",
    "        x = x[0].split('springframework')\n",
    "        if len(x) != 2:\n",
    "            x = x[0].split('wildfly')\n",
    "            if len(x) == 2:\n",
    "                x = 'org.wildfly' + x[1]\n",
    "        else:\n",
    "             x = 'org.springframework' + x[1]\n",
    "    else:\n",
    "         x = 'org.apache' + x[1]\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "   \n",
    "    x = str(x).replace(\"\\\\\",\".\")\n",
    "    return x\n",
    "\n",
    "# apply the fixes to the filename \n",
    "def clean_sc_filepath(df):\n",
    "    df.filename = df.filename.apply(format_sc_filename)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting composite words\n",
    "def findCompositeWords(s):\n",
    "    return ' '.join(re.findall('[A-Z][^A-Z]*', s))   \n",
    "\n",
    "\n",
    "def clean_composite_source_code(df):\n",
    "    df.unprocessed_code = df.unprocessed_code.apply(findCompositeWords)\n",
    "    return df\n",
    "\n",
    "def clean_composite_bug_report(df):\n",
    "    df.summary = df.summary.apply(findCompositeWords)\n",
    "    df.description = df.description.apply(findCompositeWords)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# add the Java key words to the stop words\n",
    "java_keywords = [\"abstract\", \"assert**\",\"assert\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\", \"const\", \"continue\", \"default\", \"do\", \"double\", \"else\", \"enum\", \"enum****\" \"extends\", \"final\", \"finally\", \"for\", \"goto\",\"goto*\", \"if\", \"implements\", \"import\", \"instanceof\", \"int\",\"interface\", \"long\", \"native\", \"new\", \"package\", \"private\", \"protected\", \"public\", \"return\", \"short\", \"static\", \"strictfp**\",\"strictfp\", \"super\", \"switch\", \"synchornized\", \"this\", \"throw\", \"throws\", \"transient\", \"try\", \"void\", \"volatile\", \"while\"]\n",
    "java_operators = [\"+\", \"-\", \"*\", \"/\", \"%\", \"+=\", \"-=\", \"*=\", \"/=\", \"++\", \"--\", \"==\", \"!=\", \"<\", \">\", \"<=\", \">=\", \".\", \"[\", \"]\", \"(\",\")\", \"!\", \"~\",\"instanceof\", \"<<\", \">>\", \">>>\", \"&\", \"^\", \"|\", \"&&\", \"||\", \"?\", \":\", \"^=\", \"%=\", \"<<=\", \">>=\", \">>>=\", \"&=\"]\n",
    "stop = java_keywords + java_operators\n",
    "STOP_WORDS = ENGLISH_STOP_WORDS.union(stop)\n",
    "# STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start working on TF-IDF and Cosine similarity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COLLECTIONS' 'CONFIGURATION' 'IO' 'LANG' 'DATACMNS' 'DATAMONGO'\n",
      " 'DATAREST' 'LDAP' 'SEC' 'SOCIALFB' 'SPR' 'ELY']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>unprocessed_code</th>\n",
       "      <th>project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.commons.collections.arraystack.java</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.commons.collections.bag.java</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache.commons.collections.bagutils.java</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache.commons.collections.beanmap.java</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache.commons.collections.bidimap.java</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>org.apache.commons.collections.set.testtransfo...</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>org.apache.commons.collections.set.testtypedse...</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>org.apache.commons.collections.set.testtypedso...</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>org.apache.commons.collections.set.testunmodif...</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>org.apache.commons.collections.set.testunmodif...</td>\n",
       "      <td>Licensed to the  Apache  Software  Foundation ...</td>\n",
       "      <td>COLLECTIONS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename  \\\n",
       "0       org.apache.commons.collections.arraystack.java   \n",
       "1              org.apache.commons.collections.bag.java   \n",
       "2         org.apache.commons.collections.bagutils.java   \n",
       "3          org.apache.commons.collections.beanmap.java   \n",
       "4          org.apache.commons.collections.bidimap.java   \n",
       "..                                                 ...   \n",
       "471  org.apache.commons.collections.set.testtransfo...   \n",
       "472  org.apache.commons.collections.set.testtypedse...   \n",
       "473  org.apache.commons.collections.set.testtypedso...   \n",
       "474  org.apache.commons.collections.set.testunmodif...   \n",
       "475  org.apache.commons.collections.set.testunmodif...   \n",
       "\n",
       "                                      unprocessed_code      project  \n",
       "0    Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "1    Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "2    Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "3    Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "4    Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "..                                                 ...          ...  \n",
       "471  Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "472  Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "473  Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "474  Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "475  Licensed to the  Apache  Software  Foundation ...  COLLECTIONS  \n",
       "\n",
       "[476 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# clean up the unprocessed code column\n",
    "def clean_source_df(df):\n",
    "    df = clean_new_lines_source_code(df)\n",
    "    df = clean_composite_source_code(df)\n",
    "    df = clean_sc_filepath(df)\n",
    "    return df\n",
    "\n",
    "# add the summary and description together and clean the data\n",
    "def clean_combine_bug_df(df):\n",
    "    df = clean_new_lines_bug_report(df)\n",
    "    df = clean_composite_bug_report(df)\n",
    "    df[\"query\"] = df[\"summary\"] + df[\"description\"]\n",
    "    return df\n",
    "\n",
    "# get clean versions of the dataframes\n",
    "sc_df = clean_source_df(all_projects_source_codes)\n",
    "br_df = clean_bug_df(all_projects_bugreports)\n",
    "\n",
    "\n",
    "# get a list of the projects \n",
    "print(sc_df.project.unique())\n",
    "projects = sc_df.project.unique()\n",
    "\n",
    "# group the data frames\n",
    "# TODO ignore index here? Then maintain original index and concat the data frames\n",
    "sc_grouped_df = sc_df.groupby(sc_df.project)\n",
    "bg_grouped_df = br_df.groupby(br_df.project)\n",
    "# example of getting a data frame for a single project\n",
    "col_df = sc_grouped_df.get_group(\"COLLECTIONS\")\n",
    "display(col_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def calculate_similarity(src_vect, query):\n",
    "    cosSim = cosine_similarity(query, src_vect).flatten()\n",
    "    return cosSim\n",
    "\n",
    "# fit the vectorizer and transform data\n",
    "def get_similarity(vect, src_vect, query_data):\n",
    "    query = vect.transform(query_data)\n",
    "    similarity = calculate_similarity(src_vect,query)\n",
    "     \n",
    "    return similarity\n",
    "\n",
    "# remove the stem and stop words\n",
    "# takes in an array of strings returns an array of strings\n",
    "def stem_stop(text):\n",
    "    stemmer = PorterStemmer()   #\"english\"\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in STOP_WORDS]\n",
    "    text = list(map(lambda x: stemmer.stem(x), text))\n",
    "    text = ' '.join(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# take in the source code df for a project and a signle query return scores\n",
    "def generate_scores_list(source, query):\n",
    "    scores_list = [] \n",
    "    # get a list of strings from the data frames to be vectorized\n",
    "    source['unprocessed_code'] = source['unprocessed_code'].map(stem_stop)\n",
    "    query[\"query\"] = query[\"query\"].map(stem_stop)\n",
    "    query_str = query[\"query\"].iloc[0]\n",
    "     \n",
    "    # fit a vectorizer to the data\n",
    "    vect = TfidfVectorizer(min_df=1).fit(source['unprocessed_code'])\n",
    "    src_vect = vect.transform(source['unprocessed_code'])\n",
    "    display(src_vect)\n",
    "    \n",
    "    #get the similarity scores for the bug reports    \n",
    "    #iterate through the query\n",
    "    for q in query[\"query\"]:\n",
    "        similarity = get_similarity(vect, src_vect, [q])\n",
    "        scores_list.append(similarity)\n",
    "        # some sort of data structure (dict) to collect the queries to add it to the dataframe later\n",
    "    \n",
    "    return scores_list\n",
    "    \n",
    "def generate_all_scores():\n",
    "    \n",
    "    all_bugs = []\n",
    "    all_src = []\n",
    "    # iterate through the list of 12 projects\n",
    "    for proj in projects:\n",
    "        # create dataframes for each project\n",
    "        src_df = sc_grouped_df.get_group(proj)\n",
    "        bug_df = bg_grouped_df.get_group(proj)\n",
    "        \n",
    "        # generate the scores list\n",
    "        scores = generate_scores_list(src_df, bug_df)\n",
    "    \n",
    "        #append scores list to the bug dataframe\n",
    "        bug_df[\"sim_vect\"] = scores # the only way that the matrix is related to the src code \n",
    "                                    # is through the index.\n",
    "        \n",
    "        # CALCULATE THE MAP AND MRR HERE WITH A FUNCTION AND ADD IT TO THE BUGS DATAFRAME\n",
    "        \n",
    "        # maintain a list of all the dataframes\n",
    "        all_bugs.append(bug_df)\n",
    "        all_src.append(src_df)\n",
    "    # concatenate all the data frames in order    \n",
    "    all_bug_df = pd.concat(all_bugs, ignore_index=True)\n",
    "    all_src_df = pd.concat(all_src, ignore_index=True)\n",
    "    return all_bug_df, all_src_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list\n",
    "- Figure out index of the top 20 in the bug sim_vect\n",
    "- Figure out index of files where the fix was ( this is in the bug report )\n",
    "- Calculate MAP and MRR for each\n",
    "- Generate graphs and report on the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
